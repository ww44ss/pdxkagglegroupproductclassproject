b$X.1<-NULL
b$X<-NULL
b$frequency<-NULL
b$root_freq<-NULL
colnames(a)[4]<-"log_cond_prob"
colnames(b)[4]<-"log_cond_prob"
a
b
a<-complete.cases(a)
a
a<-predictor_word(sample_phrase)
a<-a[complete.cases(a),]
a
b<-b[complete.cases(b),]
astore<-a
bstore<-b
b
b<-predictor_function(sample_phrase)
b<-b[complete.cases(b),]
astore<-a
bstore<-b
astore
three_gram[8100:8200,]
three_gram[8150:8170,]
p_3<-three_gram[8150:8180,]
p_3$log_cond_prob<-p_3$frequency-p_3$root_freq
p <- ggplot(p_3, aes(x=n_gram, y = log_cond_prob, color=(root_freq)))
p <- p + geom_line()
library(ggplot2)
p_3<-three_gram[8150:8180,]
p_3$log_cond_prob<-p_3$frequency-p_3$root_freq
p <- ggplot(p_3, aes(x=n_gram, y = log_cond_prob, color=(root_freq)))
p <- p + geom_line()
p
p_3<-three_gram[8150:8170,]
p_3$log_cond_prob<-p_3$frequency-p_3$root_freq
p <- ggplot(p_3, aes(x=n_gram, y = log_cond_prob, color=(root_freq)))
p <- p + geom_point(size=3)
p
astore
p <- ggplot(astore, aes(x=root, y = cond, color=(frequency)))
p <- p + geom_point(size=3)
p
p <- ggplot(astore, aes(x=root, y = cond, color=(frequency)))
p <- p + geom_bar()
p
?bar_plot
p <- ggplot(astore, aes(x=root, y = cond, color=(frequency)))
p <- p + geom_bar(stat="identity")
p
p <- ggplot(astore, aes(x=root, y = 10^cond, color=(frequency)))
p <- p + geom_bar(stat="identity")
p
p <- ggplot(astore, aes(x=root, y = 10^6*10^cond, color=(frequency)))
p <- p + geom_bar(stat="identity")
p
p <- ggplot(astore, aes(x=root, y = 1cond, color=(frequency)))
p <- ggplot(astore, aes(x=root, y = cond, fill=(frequency)))
p <- p + geom_bar(stat="identity") + coord_flip()
p
astore
colnames(astore)[8]<-"log_cond_prob"
colnames(bstore)[8]<-"log_cond_prob"
p <- ggplot(astore, aes(x=root, y = log_cond_prob, fill=(frequency)))
p <- p + geom_bar(stat="identity") + coord_flip()
p
p <- ggplot(astore, aes(x=root, y = log_cond_prob, fill=(root_freq)))
p <- p + geom_bar(stat="identity") + coord_flip()
p <- p + ggtitle(paste("conditional probabilities for ", store$stem[1]))
p <- p + ggtitle(paste("conditional probabilities for ", astore$stem[1]))
p
a<-predictor_word("that is the best cake ")
a
b<-predictor_function("that is the best cake ")
b
b<-predictor_function("my country tis of thee sweet ")
b
a<-predictor_word("my country tis of thee sweet ")
a
a<-predictor_word("she drove her car to the store ")
a
a<-predictor_function("she drove her car to the store ")
a
b<-predictor_function("she drove her car to the store and ")
b
a<-predictor_word("she drove her car to the store and ")
a
bstore<-b
astore<-a
bstore<-b
colnames(astore)[8]<-"log_cond_prob"
colnames(bstore)[8]<-"log_cond_prob"
print(xtable(as.data.frame(b)), type="html", include.rownames = FALSE)
require(xtable)
print(xtable(as.data.frame(a)), type="html", include.rownames = FALSE)
print(xtable(as.data.frame(b)), type="html", include.rownames = FALSE)
p <- ggplot(astore, aes(x=root, y = log_cond_prob, fill=(root_freq)))
p <- p + geom_bar(stat="identity") + coord_flip()
p <- p + ggtitle(paste("conditional probabilities for ", astore$stem[1]))
p
p <- ggplot(bstore, aes(x=root, y = log_cond_prob, fill=(root_freq)))
p <- p + geom_bar(stat="identity") + coord_flip()
p <- p + ggtitle(paste("conditional probabilities for ", astore$stem[1]))
p
astore
sample_phrase <- "we are in the best  "
a<-predictor_word(sample_phrase)
b<-predictor_function(sample_phrase)
a<-a[complete.cases(a),]
b<-b[complete.cases(b),]
astore<-a
bstore<-b
colnames(astore)[8]<-"log_cond_prob"
colnames(bstore)[8]<-"log_cond_prob"
abstore<-rbind(astore, bstore)
p <- ggplot(abstore, aes(x=root, y = log_cond_prob, fill=(root_freq)))
p <- p + geom_bar(stat="identity") + coord_flip()
p <- p + facet_grid(.~stem)
p <- p + ggtitle(paste("conditional probabilities"))
p
sample_phrase <- "Do I need to have  "
a<-predictor_word(sample_phrase)
directory <- "/Users/winstonsaunders/Documents/Coursera Swiftkey Data/final/en_US/markov_n_grams/"
four_gram  <<- read.csv(paste0(directory, "markov_four_gram.csv"))
three_gram <<-read.csv(paste0(directory, "markov_three_gram.csv"))
two_gram   <<-read.csv(paste0(directory, "markov_two_gram.csv"))
one_gram   <<-read.csv(paste0(directory, "markov_one_gram.csv"))
three_gram_X <<- read.csv(paste0(directory, "markov_three_gram_X.csv"))
two_gram_X   <<- read.csv(paste0(directory, "markov_two_gram_X.csv"))
one_gram_X   <<-read.csv(paste0(directory, "markov_one_gram_X.csv"))
last_three_words <- function(x = "you didnt enter anything") {
x<- gsub("[[:punct:]]", "", x)
text_a<-strsplit(x, " ")[[1]]  ## get the actual words
i = length(text_a)
if (i>=1) {y<-paste(text_a[i])
} else y<-"blank"
if (i>=2) {y<-paste(text_a[i-1],text_a[i])
}
if (i>=3) {y<-paste(text_a[i-2],text_a[i-1],text_a[i])
}
return(y)
}
## PREDICTION FUNCTION BASED ON WORD N-GRAMS
predictor_word <- function(full_text = "my music is") {
full_text_words<-strsplit(tolower(full_text), " ")[[1]]
text<-last_three_words(full_text)
text_split<<-strsplit(text, " ")[[1]]  ## get the actual words
## Split text into individual words
len2<-length(full_text_words)
len<-length(text_split)
## Match words
i = min(3,len)
ppflag<-1
if (i==3) {text_4<-paste(text_split[len-2],text_split[len-1],text_split[len])
pp <- four_gram[four_gram$stem==text_4,]
if(dim(pp)[1]==0) ppflag<-0
}
else ppflag<-0
qflag<-1
if (i>=2) {text_3<-paste(text_split[len-1],text_split[len])
q <- three_gram[three_gram$stem==text_3,]
if (dim(q)[1]==0) qflag<-0
}
else qflag<-0
rflag<-1
if (i>=1) {text_2<-paste(text_split[len])
r <- two_gram[two_gram$stem==text_2,]
if (dim(r)[1]==0) rflag<-0
}
else rflag<-0
## default predicted word
## if all else fails flip a coin between most probable words
if (rbinom(n=1,size=1,prob=0.35) ==1) predicted_word<-"the"
else if (rbinom(n=1,size=1,prob=0.52) ==1) predicted_word<-"to"
else predicted_word<-"and"
# Use back-off to tune predict word choice
if (rflag==1){
r$cond<-r$frequency - r$root_freq
r<-r[order(r$cond, decreasing=TRUE),]
predicted_word<-r$root[1]
predicted_phrase<-r[1:8,]
}
if (qflag==1){
q$cond<-q$frequency - q$root_freq
q<-q[order(r$cond, decreasing=TRUE),]
predicted_word<-q$root[1]
predicted_phrase<-q[1:8,]
}
if (ppflag==1){
pp$cond<-pp$frequency - pp$root_freq
pp<-pp[order(pp$cond, decreasing=TRUE),]
predicted_word<-pp$root[1]
predicted_phrase<-pp[1:8,]
}
return(predicted_phrase)
}
## PREDICTION FUNCTION BASED ON NON-STOP WORDS (Context)
##
predictor_function <- function(full_text = "my music is") {
full_text_words<-strsplit(tolower(full_text), " ")[[1]]
text_split<-full_text_words[full_text_words %in% one_gram_X$n_gram]
## Split text into individual words
len<-length(text_split)
## Match words
i = min(2,len)
qflag<-1
if (i>=2) {text_3<-paste(text_split[len-1],text_split[len])
q <- three_gram_X[three_gram_X$stem==text_3,]
if (dim(q)[1]==0) qflag<-0
}
else qflag<-0
rflag<-1
if (i>=1) {text_2<-paste(text_split[len])
r <- two_gram_X[two_gram_X$stem==text_2,]
if (dim(r)[1]==0) rflag<-0
}
else rflag<-0
## default predicted word
## if all else fails flip a coin between most probable words
if (rbinom(n=1,size=1,prob=0.35) ==1) predicted_word<-"will"
else if (rbinom(n=1,size=1,prob=0.52) ==1) predicted_word<-"said"
else predicted_word<-"just"
# Use back-off to tune predict word choice
if (rflag==1){
r$cond<-r$frequency - r$root_freq
r<-r[order(r$cond, decreasing=TRUE),]
predicted_word<-r$root[1]
predicted_phrase<-r[1:8,]
}
if (qflag==1){
q$cond<-q$frequency - q$root_freq
q<-q[order(r$cond, decreasing=TRUE),]
predicted_word<-q$root[1]
predicted_phrase<-q[1:8,]
}
return(predicted_phrase)
}
sample_phrase <- "Do I need to have  "
a<-predictor_word(sample_phrase)
b<-predictor_function(sample_phrase)
a<-a[complete.cases(a),]
b<-b[complete.cases(b),]
astore<-a[1:7,]
bstore<-b[1:7,]
colnames(astore)[8]<-"log_cond_prob"
colnames(bstore)[8]<-"log_cond_prob"
a$X.1<-NULL
a$X<-NULL
a$frequency<-NULL
a$root_freq<-NULL
b$X.1<-NULL
b$X<-NULL
b$frequency<-NULL
b$root_freq<-NULL
colnames(a)[4]<-"log_cond_prob"
colnames(b)[4]<-"log_cond_prob"
a<-a[1:3,]
b<-b[1:3,]
a
astore
a<-predictor_word(sample_phrase)
sample_phrase
predictor_word(sample_phrase)
full_text<-sample_phrase
full_text_words<-strsplit(tolower(full_text), " ")[[1]]
full_text_words
text<-last_three_words(full_text)
text_split<<-strsplit(text, " ")[[1]]  ## get the actual words
text_split
sample_phrase <- "Do I need to have"
full_text<-sample_phrase
full_text_words<-strsplit(tolower(full_text), " ")[[1]]
full_text_words
text<-last_three_words(full_text)
text
text_split<<-strsplit(text, " ")[[1]]  ## get the actual words
## Split text into individual words
len2<-length(full_text_words)
len<-length(text_split)
len
len2
i = min(3,len)
ppflag<-1
i
if (i==3) {text_4<-paste(text_split[len-2],text_split[len-1],text_split[len])
pp <- four_gram[four_gram$stem==text_4,]
if(dim(pp)[1]==0) ppflag<-0
}
else ppflag<-0
pp
qflag<-1
if (i>=2) {text_3<-paste(text_split[len-1],text_split[len])
q <- three_gram[three_gram$stem==text_3,]
if (dim(q)[1]==0) qflag<-0
}
else qflag<-0
qflag<-1
if (i>=2) {text_3<-paste(text_split[len-1],text_split[len])
q <- three_gram[three_gram$stem==text_3,]
if (dim(q)[1]==0) qflag<-0
} else qflag<-0
rflag<-1
if (i>=1) {text_2<-paste(text_split[len])
r <- two_gram[two_gram$stem==text_2,]
if (dim(r)[1]==0) rflag<-0
} else rflag<-0
## default predicted word
## if all else fails flip a coin between most probable words
if (rbinom(n=1,size=1,prob=0.35) ==1) predicted_word<-"the"
else if (rbinom(n=1,size=1,prob=0.52) ==1) predicted_word<-"to"
else predicted_word<-"and"
if (rbinom(n=1,size=1,prob=0.35) ==1) predicted_word<-"the" else
if (rbinom(n=1,size=1,prob=0.52) ==1) predicted_word<-"to" else
predicted_word<-"and"
if (rflag==1){
r$cond<-r$frequency - r$root_freq
r<-r[order(r$cond, decreasing=TRUE),]
predicted_word<-r$root[1]
predicted_phrase<-r[1:8,]
}
predicted_phrase
if (qflag==1){
q$cond<-q$frequency - q$root_freq
q<-q[order(r$cond, decreasing=TRUE),]
predicted_word<-q$root[1]
predicted_phrase<-q[1:8,]
}
predicted_phrase
if (ppflag==1){
pp$cond<-pp$frequency - pp$root_freq
pp<-pp[order(pp$cond, decreasing=TRUE),]
predicted_word<-pp$root[1]
predicted_phrase<-pp[1:8,]
}
print(-------)
print("-------")
print(rflag)
print(ppflag)
print(qflag)
return(predicted_phrase)
predictor_word(sample_phrase)
sample_phrase <- "Do I need to have"
a<-predictor_word(sample_phrase)
b<-predictor_function(sample_phrase)
a<-a[complete.cases(a),]
b<-b[complete.cases(b),]
astore<-a[1:7,]
astore
astore<-a
bstore<-b
astore
abstore<-rbind(astore, bstore)
p <- ggplot(abstore, aes(x=root, y = log_cond_prob, fill=(root_freq)))
p <- p + geom_bar(stat="identity") + coord_flip()
p <- p + facet_grid(.~stem)
p <- P + scale_color_brewer(palette="Set1")
p <- p + ggtitle(paste("conditional probs for word- and context- prediction"))
p
sample_phrase <- "Theres a lady who's sure all that"
#p <- p + scale_color_gradient2(midpoint=-1000, low="blue", mid="white", high="red" )
source('~/Documents/pdxkagglegroupproductclassproject/explore1.R')
dim(complete.cases(train_data))==dim(train_data)
dim(complete.cases(train_data))
dim(train_data)
a<-is.na(train_data)
a
table(a)
source('~/Documents/pdxkagglegroupproductclassproject/explore1.R')
head(train_morph)
dim(train_morph)
dim(train_data)
train_data<-read.csv(paste0(directory,file_name))
dim(train_data)
```{r, fig.width=12, fig.height=4, echo=FALSE}
source('~/Documents/pdxkagglegroupproductclassproject/explore1.R')
R.version.string
install.packages("manipulate")
install.packages("knitr")
source('~/Documents/pdxkagglegroupproductclassproject/explore1.R')
?gather
??gather
require(plyr)
install.packages("plyr")
install.package(tidyr)
install.packages(tidyr)
install.packages("tidyr")
install.packages("plyr")
install.packages("ggplot2")
install.packages("xtable")
class_list<-train_morph["target"== "Class_1",]
```
</small>
p <- p + facet_wrap(target, nrow=3)
source('~/Documents/pdxkagglegroupproductclassproject/explore1.R')
source('~/Documents/pdxkagglegroupproductclassproject/explore1.R')
source('~/Documents/pdxkagglegroupproductclassproject/explore1.R')
source('~/Documents/pdxkagglegroupproductclassproject/explore1.R')
source('~/Documents/pdxkagglegroupproductclassproject/explore1.R')
source('~/Documents/pdxkagglegroupproductclassproject/explore1.R')
directory<- "/Users/winstonsaunders/Documents/pdxkagglegroupproductclassproject/"
file_name<- "train.csv"
train_data<-read.csv(paste0(directory,file_name))
print(head(train_data))
library(rpart)
###
fit<-rpart(target~.-target-id, method="class",
data=train_data[train_data$target=="Class_1"|train_data$target=="Class_2",])
plot(fit)
plot(fit)
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
plot(fit, uniform=TRUE,
main="Classification Tree for Kyphosis")
plot(fit, uniform=TRUE,
main="Classification Tree for Product Class")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
plot(fit, uniform=TRUE,
main="Classification Tree for Product Class")
text(fit, use.n=TRUE, all=TRUE, cex=.5)
text(fit, use.n=TRUE, all=TRUE, cex=.9)
```{r, fig.width=12, fig.height=6, fig.align="center"}
```{r, echo=7:8}
install.packages("caret")
install.packages("caret", dependencies = c("Depends", "Suggests"))
install.packages("tree")
install.packages("treemap")
source('~/Documents/pdxkagglegroupproductclassproject/explore1.R')
shiny::runApp('Coursera_Shiny_Capstone2')
?head
aa<-c(1:10, 5:6, 4:8, 5, 5, 5, 8, 12)
table(aa)
colnames(aa)
bb<-table(aa)
colnames(bb)
bb
str(bb)
cc<-as.data.frame(bb)
cc
t(cc)
bb
t(bb)
as.data.frame(t(bb))
?tqble
?table
print(table(f_7_table))
setwd("/Users/winstonsaunders/Documents/pdxkagglegroupproductclassproject/")
## Get the data
directory<- "/Users/winstonsaunders/Documents/pdxkagglegroupproductclassproject/"
file_name<- "train.csv"
train_data<-read.csv(paste0(directory,file_name))
dd<-dim(train_data)
require(xtable)
print(xtable(train_data[1:4, c(1:6, 93:95)]), type="html",include.rownames = FALSE)
print(t(table(train_data$target)), type="html",include.rownames = FALSE)
sample_data<-TRUE
set.seed(8675309)
if (sample_data == TRUE){
feats<-colnames(train_data)
feats<-feats[3:93]
sample_rows <- sample(1:dim(train_data)[1], 15000)
#sample_columns<-c("id","feat_1", sample(feats, 28), "feat_93", "target")
train_data_T <- train_data[sample_rows, ]#sample_columns]
dd<-dim(train_data_T)
## assign subset to data frame name temporarily
train_data<-train_data_T ## remove this for full analysis
}
require(xtable)
f_6_table<-train_data[train_data$target=="Class_6", "feat_60"]
table(f_6_table)
hist(f_7_table)
f_6_table<-train_data[train_data$target=="Class_6", "feat_60"]
hist(f_6_table)
?hist
hist(f_6_table, breaks=30)
f_7_table<-train_data[train_data$target=="Class_7", "feat_60"]
hist(f_7_table, breaks=30)
require(plyr); require(ggplot2); require(tidyr)
## munge data into long format
long_train<-gather(train_data, feature, data, feat_1:feat_93)
dim(long_train)
head(long_train)
dim(train_data)
aa<-dim(train_data)
aa[1]*aa[2]
aa[1]*(aa[2]-1)
aa[1]*(aa[2]-2)
hist(long_train$data)
?hist
install.packages("tree")
head(train_morph)
head(long_train)
source('~/Documents/pdxkagglegroupproductclassproject/explore1.R')
long_train<-gather(train_data, feature, data, feat_1:feat_93)
print(head(long_train,12))
train_morph<-ddply(long_train, c("target", "feature"), summarize, mean_data = mean(data), sdev_data = sqrt(var(data)))
## calculate the z-stat
train_morph<-ddply(long_train, c("target", "feature"), summarize, mean_data = mean(data), sdev_data = sqrt(var(data)))
## calculate the z-stat
train_morph$CV<-train_morph$sdev_data/(train_morph$mean_data+.00001)
head(train_morph)
p<-ggplot(train_morph, aes(x=target, y=mean_data, color=feature))+geom_point()+ theme_bw()
p <- p + ggtitle("means of several features versus class")
p <- p + guides(color=guide_legend(nrow=10))
print(p)
p<-ggplot(train_morph, aes(x=target, y=mean_data, color=feature))+geom_point()+ theme_bw()
print(p)
str(train_morph)
source('~/Documents/pdxkagglegroupproductclassproject/explore1.R')
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
