Product Classification
========================================================
transition: rotate
  
  
<br>  
<br>  
_Winston Saunders_   
_April 22, 2015_  
__Exploratory Summary__
 
```{r, echo=FALSE}
setwd("/Users/winstonsaunders/Documents/pdxkagglegroupproductclassproject/")
```
 <br>
<small>`r R.version.string`</small>

***
<br>  
<br>  
![alt text](tactical.001.jpg)


Grabbing the data
========================================================

```{r, echo=FALSE}
        ## Get the data
        directory<- "/Users/winstonsaunders/Documents/pdxkagglegroupproductclassproject/"
        file_name<- "train.csv"
        
        train_data<-read.csv(paste0(directory,file_name))
        dd<-dim(train_data)

```

Data are stored locally on my computer in the directory `r print(directory)`.
The train data set has `r dd[1]` rows and `r dd[2]` columns. Here is a sample of a few rows and columns. The target column has 9 classifiers.  
<small>
```{r, echo=FALSE, results='asis' }
require(xtable)

print(xtable(train_data[1:4, c(1:6, 93:95)]), type="html",include.rownames = FALSE)
```
</small>  
The number of elements in each class is not uniform. 
<small>   
```{r, echo=FALSE, results='asis' }
print(xtable(t(table(train_data$target))), type="html",include.rownames = FALSE)
```
</small> 



Create data sample
========================================================
To get some data for inspection first create a smaller random sample of rows. This just speeds up calculations. 

```{r, echo=FALSE}
sample_data<-TRUE
        
        set.seed(8675309)    
        if (sample_data == TRUE){
                feats<-colnames(train_data)
                feats<-feats[3:93]
        
                        sample_rows <- sample(1:dim(train_data)[1], 15000)  
                        #sample_columns<-c("id","feat_1", sample(feats, 28), "feat_93", "target")
        
                        train_data_T <- train_data[sample_rows, ]#sample_columns]
                        dd<-dim(train_data_T)
        
                        ## assign subset to data frame name temporarily
        
                        train_data<-train_data_T ## remove this for full analysis
        }  
```

> The sampled train_data has `r dd[1]` rows and `r dd[2]` columns. 

```{r, echo=FALSE, results='asis' }
require(xtable)

print(xtable(train_data[1:4,c(1:7,dd[2]) ]), type="html",include.rownames = FALSE)
```

Histograms of Features
======================================

What does a specific feat look like? Look at histogram of values for feat\_60 for Class\_6

```{r, echo=FALSE, results='asis'}
f_6_table<-train_data[train_data$target=="Class_6", "feat_60"]
hist(f_6_table, breaks=30)

```
***

```{r, echo=FALSE, results='asis'}

f_7_table<-train_data[train_data$target=="Class_7", "feat_60"]
hist(f_7_table, breaks=30)
        
```


Munging
=========================================
For plotting etc. convert data to a "long" format with each observation, characterized by a target class and feature, in one unique row.
<small>
```{r}
## Get packages
require(plyr); require(ggplot2); require(tidyr)
## munge data into long format 
long_train<-gather(train_data, feature, data, feat_1:feat_93)
```

Note that the dimension of this new data frame `r dim(long_train)[1]` is equal to `r dim(train_data)[1]` X (`r dim(train_data)[2]` - 2).

Here is a sample...   (the table has `r dim(long_train)[1]` rows)
```{r, echo=FALSE, results='asis' }
require(xtable)
print(xtable(head(long_train,4)), type="html",include.rownames = FALSE)
```
</small>  
First summary
============================================

Let's look at the means and std deviations of the features by class....

<small>
```{r}
## use ddply to get means and standard deviations
train_morph<-ddply(long_train, c("target", "feature"), summarize, mean_data = mean(data), sdev_data = sqrt(var(data)))
## calculate coeff of variation (CV)
## adding small value to prevent overflow errors
train_morph$CV<-(train_morph$sdev_data)/(train_morph$mean_data+0.001)

```
</small>

Inspect train_morph
======================================

```{r,results='asis' }
print(xtable(train_morph[1:6,]), type="html",include.rownames = FALSE)

```

Let's inspect
=========================================

```{r, fig.width=12, fig.height=3, echo=FALSE}
 p<-ggplot(train_morph, aes(x=target, y=mean_data, color=feature))+geom_point(size=2)+ theme_bw()
p <- p + ggtitle("means of several features versus class")
p <- p + guides(color=guide_legend(nrow=10))
p <- p + theme(axis.text.x = element_text(size=rel(1.5)), axis.text.y = element_text(size=rel(1.5)))
p <- p + theme(axis.title.x = element_text(size=rel(1.5)), axis.title.y = element_text(size=rel(1.5)))
p <- p + theme(plot.title = element_text(size=rel(1.5)))
print(p)
```

Here is the CV = sd/mean

```{r, fig.width=12, fig.height=3, echo=FALSE}
p<-ggplot(train_morph, aes(x=target, y=CV, color=feature))+geom_point(size=3)+ theme_bw()
        p <- p + ggtitle("mean/sd of several features versus class")
        p <- p + guides(color=guide_legend(nrow=10))
        print(p)
```

>some variation with class, but low mean/sd = 1/CV means noisy correlations!

1/CV features for each class
===================================

```{r, fig.width=9, fig.height=6, fig.align="center",echo=FALSE}

p <- ggplot(train_morph, aes(x=feature, y=z_stat, color=feature))+geom_point(size=3)+ theme_bw()
        p <- p + ggtitle("1/CV of several features versus feature")
        p <- p + theme(axis.text.x = element_text(angle = 90, hjust = 1))
        p <- p + facet_grid(target~.)
        p <- p + guides(color=FALSE)
        print(p)
```

This is beginning to look promising. We can see at least some variation between classes & features (though others are weak)

</small>  
Second Summary
============================================

Let's get rid of all the data that is <= 0.

```{r}
long_train_original<-long_train
long_train<-long_train[long_train$data>0,]
```

the size of the original matrix was `r dim(long_train_original)[1]` and the size of the new matrix is `r dim(long_train)[1]`.

<small>
```{r, echo=FALSE}
## use ddply to get means and standard deviations
train_morph<-ddply(long_train, c("target", "feature"), summarize, mean_data = mean(data), sdev_data = sqrt(var(data)))


## calculate inverse coeff of variation 
## (which I will label as z_stat for later use)
## add small value to prevent overflow errors
train_morph$z_stat<-train_morph$mean_data/(train_morph$sdev_data+0.00001)
train_morph$CV<-(train_morph$sdev_data)/(train_morph$mean_data+0.001)

```
</small>

Train_Morph the new data
======================================

```{r,results='asis' }
print(xtable(train_morph[1:6,]), type="html",include.rownames = FALSE)

```




Means and CV for all classes
=========================================

```{r, fig.width=12, fig.height=3, echo=FALSE}
 p<-ggplot(train_morph, aes(x=target, y=mean_data, color=feature))+geom_point(size=2)+ theme_bw()
p <- p + ggtitle("means of several features versus class")
p <- p + guides(color=guide_legend(nrow=10))
p <- p + theme(axis.text.x = element_text(size=rel(1.5)), axis.text.y = element_text(size=rel(1.5)))
p <- p + theme(axis.title.x = element_text(size=rel(1.5)), axis.title.y = element_text(size=rel(1.5)))
p <- p + theme(plot.title = element_text(size=rel(1.5)))
print(p)
```

Here is the CV = sd/mean

```{r, fig.width=12, fig.height=3, echo=FALSE}
p<-ggplot(train_morph, aes(x=target, y=CV, color=feature))+geom_point(size=3)+ theme_bw()
        p <- p + ggtitle("mean/sd of several features versus class")
        p <- p + guides(color=guide_legend(nrow=10))
        print(p)
```

>some variation with class, but low mean/sd = 1/CV means noisy correlations!

1/CV features for each class
===================================

```{r, fig.width=9, fig.height=6, fig.align="center",echo=FALSE}

p <- ggplot(train_morph, aes(x=feature, y=z_stat, color=feature))+geom_point(size=3)+ theme_bw()
        p <- p + ggtitle("1/CV of several features versus feature")
        p <- p + theme(axis.text.x = element_text(angle = 90, hjust = 1))
        p <- p + facet_grid(target~.)
        p <- p + guides(color=FALSE)
        print(p)
```

This is beginning to look promising. We can see at least some variation between classes & features (though others are weak)


Another look
==================================

Here the mean and 1/CV within a product class is plotted with color coded for the target class. 

```{r, fig.width=9, fig.height=6, fig.align="center", echo=FALSE}

p <- ggplot(train_morph, aes(x=mean_data, y=z_stat, color=target))+geom_point(size=5)+ theme_bw()
        p <- p + ggtitle("1/CV versus mean of several features")
        p <- p + theme(axis.text.x = element_text(angle = 90, hjust = 1))
        #p <- p + facet_wrap(~target, nrow=3)
        p <- p + guides(color=guide_legend(nrow=10))
        print(p)
```


Another look
==================================

If we require that 1/CV be above 0.5 (arbitrarily)
```{r, fig.width=12, fig.height=6, fig.align="center",echo=FALSE}

p <- ggplot(train_morph[train_morph$z_stat>0.5,], aes(x=mean_data, y=z_stat, color=feature))+geom_point(size=5)+ theme_bw()
        p <- p + ggtitle("z_stat versus mean of several features")
        p <- p + theme(axis.text.x = element_text(angle = 90, hjust = 1))
        p <- p + facet_wrap(~target, nrow=3)
        p <- p + guides(color=guide_legend(nrow=10))
        print(p)
```

>This starts to look selective  


Decision Tree Class1 vs Class2 
===================================

Decision trees also appear to offer a good way to distinguish.

```{r, fig.width=12, fig.height=6, fig.align="center",echo=FALSE}
library(rpart)

        fit<-rpart(target~.-target-id, method="class", 
                   data=train_data[train_data$target=="Class_1"|train_data$target=="Class_2",])


        plotcp(fit)
```

Tree Plot (pruned to cp=0.02)
===================================

This shows just a few features (e.g. 17, 78, and 84) can do a pretty good job discriminating between Class\_1 and Class\_2. 


```{r, fig.width=10, fig.height=6, fig.align="center", echo=FALSE}
        
fit_cp<-prune(fit, cp=0.02)
plot(fit_cp, uniform=TRUE, 
             main="Classification Tree Class_1 Class_2")
text(fit, use.n=TRUE, all=TRUE, cex=0.9) 


```


Decision Tree Class1 vs Class6 
===================================

Here's a look at a different pairing (Class\_1 and CLass\_6)

```{r, fig.width=12, fig.height=6, fig.align="center",echo=FALSE}
library(rpart)

        fit<-rpart(target~.-target-id, method="class", 
                   data=train_data[train_data$target=="Class_1"|train_data$target=="Class_6",])


        plotcp(fit)
```

Tree Plot (pruned to cp=0.02)
===================================

This shows many more features (e.g. 8, 78, 6, &c.) are needed to discriminate between Class\_1 and Class\_6. Though feat\_78 is common between the two analyses.


```{r, fig.width=10, fig.height=6, fig.align="center", echo=FALSE}
        
fit_cp<-prune(fit, cp=0.02)
plot(fit_cp, uniform=TRUE, 
             main="Classification Tree Class_1 Class_2")
text(fit, use.n=TRUE, all=TRUE, cex=0.9) 


```

Next Steps
=========================================

Examine more pairwise trees.  
Understand if the variable _values_ contribute information.   
Run generic random forest as benchmark.  

>Cheers!



