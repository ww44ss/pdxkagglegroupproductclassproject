Product Classification
========================================================
transition: rotate
  
  
<br>  
<br>  
_Winston Saunders_   
_April 22, 2015_  
__Exploratory Summary__
 
```{r, echo=FALSE}
setwd("/Users/winstonsaunders/Documents/pdxkagglegroupproductclassproject/")
```
 <br>
<small>`r R.version.string`</small>

***
<br>  
<br>  
![alt text](tactical.001.jpg)


Grabbing the data
========================================================

```{r, echo=FALSE}
        ## Get the data
        directory<- "/Users/winstonsaunders/Documents/pdxkagglegroupproductclassproject/"
        file_name<- "train.csv"
        
        train_data<-read.csv(paste0(directory,file_name))
        dd<-dim(train_data)

```

Data are stored locally on my computer in the directory `r print(directory)`.
The train data set has `r dd[1]` rows and `r dd[2]` columns. Here is a sample of a few rows and columns. The target column has 9 classifiers.  
<small>
```{r, echo=FALSE, results='asis' }
require(xtable)

print(xtable(train_data[1:4, c(1:6, 93:95)]), type="html",include.rownames = FALSE)
```
</small>  
The number of elements in each class is not uniform. 
<small>   
```{r, echo=FALSE, results='asis' }
print(xtable(t(table(train_data$target))), type="html",include.rownames = FALSE)
```
</small> 



Create data sample
========================================================
To get some data for inspection first create a smaller random sample of rows. This just speeds up calculations. 

```{r, echo=FALSE}
sample_data<-TRUE
        
        set.seed(8675309)    
        if (sample_data == TRUE){
                feats<-colnames(train_data)
                feats<-feats[3:93]
        
                        sample_rows <- sample(1:dim(train_data)[1], 15000)
                        test_rows<- -sample_rows
                        #sample_columns<-c("id","feat_1", sample(feats, 28), "feat_93", "target")
        
                        train_data_T <- train_data[sample_rows, ]#sample_columns]
                        test_data<-train_data[test_rows, ]
                        dd<-dim(train_data_T)
        
                        ## assign subset to data frame name temporarily
        
                        train_data<-train_data_T ## remove this for full analysis
        }  
```

> The sampled train_data has `r dd[1]` rows and `r dd[2]` columns. 

```{r, echo=FALSE, results='asis' }
require(xtable)

print(xtable(train_data[1:4,c(1:7,dd[2]) ]), type="html",include.rownames = FALSE)
```

Histograms of Features
======================================

What does a specific feat look like? Look at histogram of values for feat\_60 for Class\_6

```{r, echo=FALSE, results='asis'}
f_6_table<-train_data[train_data$target=="Class_6", "feat_60"]
hist(f_6_table, breaks=30)

```
***

```{r, echo=FALSE, results='asis'}

f_7_table<-train_data[train_data$target=="Class_7", "feat_60"]
hist(f_7_table, breaks=30)
        
```


Munging
=========================================
For plotting etc. convert data to a "long" format with each observation, characterized by a target class and feature, in one unique row.
<small>
```{r}
## Get packages
require(plyr); require(ggplot2); require(tidyr)
## munge data into long format 
long_train<-gather(train_data, feature, data, feat_1:feat_93)
```

Note that the dimension of this new data frame `r dim(long_train)[1]` is equal to `r dim(train_data)[1]` X (`r dim(train_data)[2]` - 2).

Here is a sample...   (the table has `r dim(long_train)[1]` rows)
```{r, echo=FALSE, results='asis' }
require(xtable)
print(xtable(head(long_train,4)), type="html",include.rownames = FALSE)
```
</small>  
First summary
============================================

Let's look at the means and std deviations of the features by class....

<small>
```{r}
## use ddply to get means and standard deviations
train_morph<-ddply(long_train, c("target", "feature"), summarize, mean_data = mean(data), sdev_data = sqrt(var(data)))
## calculate coeff of variation (CV)
## adding small value to prevent overflow errors
train_morph$CV<-(train_morph$sdev_data)/(train_morph$mean_data+0.001)

```
</small>

Inspect train_morph
======================================

```{r,results='asis' }
print(xtable(train_morph[1:6,]), type="html",include.rownames = FALSE)

```

Let's inspect
=========================================

```{r, fig.width=8, fig.height=3, echo=FALSE, fig.align="center"}
 p<-ggplot(train_morph, aes(x=target, y=mean_data, color=feature))+geom_point(size=2)+ theme_bw()
p <- p + ggtitle("means of several features versus class")
p<- p + theme(legend.position="none")
#p <- p + guides(color=guide_legend(nrow=10))
p <- p + theme(axis.text.x = element_text(size=rel(1.5)), axis.text.y = element_text(size=rel(1.5)))
p <- p + theme(axis.title.x = element_text(size=rel(1.5)), axis.title.y = element_text(size=rel(1.5)))
p <- p + theme(plot.title = element_text(size=rel(1.5)))
print(p)
```

Here is the CV = sd/mean

```{r, fig.width=8, fig.height=3, echo=FALSE, fig.align="center"}
p<-ggplot(train_morph, aes(x=target, y=CV, color=feature))+geom_point(size=3)+ theme_bw()
        p <- p + ggtitle("CV = sd/mean of several features versus class")
        p<- p + theme(legend.position="none")
        #p <- p + guides(color=guide_legend(nrow=10))
        print(p)
```

>some variation with class, but high CV means noisy correlations!

Another look at the data: CV for each class
===================================

```{r, fig.width=11, fig.height=6, fig.align="center",echo=FALSE}

p <- ggplot(train_morph, aes(x=feature, y=CV, color=feature))+geom_point(size=3)+ theme_bw()
        p <- p + ggtitle("CV of several features versus feature")
        p <- p + theme(axis.text.x = element_text(angle = 90, hjust = 1))
        p <- p + facet_grid(target~.)
        p<- p + theme(legend.position="none")
        #p <- p + guides(color=FALSE)
        print(p)
```

This is beginning to look promising. We can see some variation between classes & features, but overall CV is very high and needs to be reduced.


</small>  
Reducing CV
============================================

Let's try getting rid of all the data that = 0 (assume 0 is a non-entry)

```{r}
long_train_nz<-long_train[long_train$data>0,]
```

the size of the original matrix was `r dim(long_train)[1]` and the size of the new matrix is `r dim(long_train_nz)[1]`. So there were a substantial number of zeros!

```{r, echo=FALSE}
## use ddply to get means and standard deviations
train_morph_nz<-ddply(long_train_nz, c("target", "feature"), summarize, mean_data = mean(data), sdev_data = sqrt(var(data)))


## calculate inverse coeff of variation 
## (which I will label as z_stat for later use)
## add small value to prevent overflow errors
train_morph_nz$CV<-(train_morph_nz$sdev_data)/(train_morph_nz$mean_data+0.001)

```

```{r,echo=FALSE, results='asis' }
print(xtable(train_morph_nz[1:4,]), type="html",include.rownames = FALSE)

```

>Note the substantial reduction in CV

Means and CV for all classes
=========================================

```{r, fig.width=8, fig.height=3, echo=FALSE}
 p<-ggplot(train_morph_nz, aes(x=target, y=mean_data, color=feature))+geom_point(size=3)+ theme_bw()
p <- p + ggtitle("means of several features versus class")
p<- p + theme(legend.position="none")
#p <- p + guides(color=guide_legend(nrow=10))
p <- p + theme(axis.text.x = element_text(size=rel(1.5)), axis.text.y = element_text(size=rel(1.5)))
p <- p + theme(axis.title.x = element_text(size=rel(1.5)), axis.title.y = element_text(size=rel(1.5)))
p <- p + theme(plot.title = element_text(size=rel(1.5)))
print(p)
```

Here is the CV = sd/mean

```{r, fig.width=8, fig.height=3, echo=FALSE}
p<-ggplot(train_morph_nz, aes(x=target, y=CV, color=feature))+geom_point(size=3)+ theme_bw()
        p <- p + ggtitle("mean/sd of several features versus class")
        p<- p + theme(legend.position="none")
#        p <- p + guides(color=guide_legend(nrow=10))
        print(p)
```

>some variation with class, but reduced CV means less noisy correlations!

CV features for each class
===================================

```{r, fig.width=11, fig.height=6, fig.align="center",echo=FALSE}

p <- ggplot(train_morph_nz, aes(x=feature, y=CV, color=feature))+geom_point(size=3)+ theme_bw()
        p <- p + ggtitle("1/CV of several features versus feature")
        p <- p + theme(axis.text.x = element_text(angle = 90, hjust = 1))
        p <- p + facet_grid(target~.)
        p<- p + theme(legend.position="none")
        print(p)
```

This is beginning to look promising. We can see at least some variation between classes & features (though others are weak)


Another look
==================================

The non-zero data with CV < 1.5 shows fair distinguishing of classes.

```{r, fig.width=11, fig.height=6, fig.align="center", echo=FALSE}

p <- ggplot(train_morph_nz[train_morph_nz$CV<1.5,], aes(x=mean_data, y=CV, color=target))+geom_point(size=5)+ theme_bw()
        p <- p + ggtitle("CV versus mean of several features")
        p <- p + theme(axis.text.x = element_text(angle = 90, hjust = 1))
        #p <- p + facet_wrap(~target, nrow=3)
        p <- p + guides(color=guide_legend(nrow=10))
        print(p)
```



Decision Tree Class1 vs Class2 
===================================

Decision trees also appear to offer a good way to distinguish.

```{r, fig.width=12, fig.height=6, fig.align="center",echo=FALSE}
library(rpart)



        fit<-rpart(target~.-target-id, method="class", 
                   data=train_data[train_data$target=="Class_1"|train_data$target=="Class_2",])

 
        plotcp(fit)
```



Use Tree Package
===================================

This shows just a few features (e.g. 17, 78, and 84) can do a pretty good job discriminating between Class\_1 and Class\_2. 


```{r, fig.width=10, fig.height=6, fig.align="center", echo=FALSE}
        

library(tree)
 
tree_fit = tree(target~.-target-id, data=train_data)
summary(tree_fit)

```

Plot of Tree
=============================
```{r, fig.width=10, fig.height=6, fig.align="center"}
plot(tree_fit, pretty=0);text(tree_fit)
```

Check Accuracy
===========================================
```{r}
td<-test_data
td$id<-NULL
tree_pred<-predict(tree_fit, data=td, type="class")
```

Can we do tree on long_data?
===========================================

```{r}
long_tree_fit <- tree(target~.-id-target, data=long_train_nz)
summary(long_tree_fit)


```

Plot of Long_Tree
=============================
```{r, fig.width=10, fig.height=6, fig.align="center"}
plot(long_tree_fit, pretty=0);text(tree_fit)
```



